.. _sql_query-label:

ibmcloudsql.sql_query
================================================

:mod:`ibmcloudsql.sql_query` provides 1 class

* :py:class:`ibmcloudsql.sql_query.SQLClient` class

A :class:`SQLClient` class is also a :class:`COSClient` class, and a :class:`SQLMagic` class.

It also provides APIs

1. like ``ibmcloudsql.SQLQuery`` class for certain APIs, with extended arguments to provide a richer set of functionalities and better handling of queries.
It is important to know that SQLClient does not aim to implement all APIs from ibmcloudsql.SQLQuery, with the common ones
are implemented. So for those that are not implemented, we can use such APIs via its :py:attr:`.engine`

.. code-block:: python

    sqlClient = SQLQuery(....)
    sqlClient.engine.[ibmcloudsql-API]


1. :py:class:`.SQLClient` (...): instantiate with initial settings
2. :meth:`.configure` : update the setting

A. Help
------------

1. :meth:`.help`
2. :meth:`.sql_info`
3. :meth:`.get_job_demo`
4. :meth:`.get_cos_summary_demo`
5. :meth:`.list_results_demo`
6. :meth:`.analyze`: if you are stuck with a query that takes too long, try this one. It may provide you with suggestion on how to improve your query or revise your data source.

B. SQL Query
------------
0. :meth:`.engine`: return the ibmclousql engine
1. :meth:`.submit_sql`, :meth:`.submit_sql2`
2. :meth:`.run_sql`, :meth:`.run_sql2`
3. :meth:`.submit_sql_with_checking_saved_jobs`
4. :meth:`.get_schema_data`:
5. :meth:`.submit` and :meth:`.run`: the two new methods run a SQL statement generated by the approach provided by :class:`SQLMagic` class

C. Query result
---------------

1. :meth:`.get_result`
2. :meth:`.delete_result`
3. :meth:`.rename_exact_result`: modify the created objects on COS
4. :meth:`.rename_exact_result_joblist`: ... from a list of jobs
5. :meth:`.delete_zero_sized_objects`:
6. :meth:`.get_cos_summary`
7. :meth:`.list_results`


D. Jobs
------------

1. :meth:`.my_jobs`
2. :meth:`.wait_for_job`
3. :meth:`.process_failed_jobs_until_all_complete`
4. :meth:`.get_job`
5. :meth:`.get_jobs`
6. :meth:`.get_number_running_jobs`
7. :meth:`.get_jobs_matching`
8. :meth:`.export_jobs_history`


E. COS URL handling
-------------------

1. :meth:`.cos_url_parser`

F. HIVE catalog table
----------------------

1. :meth:`.get_catalog_table`
2. :meth:`.show_catalog_tables`
3. :meth:`.drop_all_catalog_tables`
4. :meth:`.drop_catalog_tables`
5. :meth:`.drop_catalog_table`
6. :meth:`.get_catalog_table`
7. :meth:`.get_catalog_table_partitioned`: for partitioned catalog table
8. :meth:`.refresh_cache_table_partitioned`:
9. :meth:`.describe_table`

G. Data skipping
----------------------

[Not available yet]

H. Prepare data for time-series
-------------------------------------

* :meth:`._get_ts_datasource` :  explaination below

Assuming a HIVE catalog table is created to store the data for fast access, which
is used as the data source via `table_name` argument.

A time-series comprises

1. `time_stamp` information
2. `observation` information
3. for what category, i.e. the `key`

Very often, the raw data are too dense to be digested into a time-series.  Such data
is then needed to be transformed into a finer time-scale, for example:

* `raw`: no change, just extract to a new location
* `per_sec`: per every second
* `per_2sec`: per every 2-second
* `per_min`: per every minute
* `per_5min`: per every 5 minute

It support 'per_[x]sec' and 'per_[x]min' with x is divisible by 60.

Such transformed data is then copied and save into a new location (the time-series data source), which is specified by

* `cos_out`: COS URL (stored as PARQUET)
* `num_objects`: split into multiple objects or
* `num_rows`: split into multiple objects based on number of rows per object

**At the end of the transformation**, the data source to be used for time-series creation comprises 3 columns:

* `field_name`: representing whatever category
* `time_stamp`: representing the time-point at the given granularity
* `observation`: representing the recorded information

The use of generic-name enables the task to be quickly applicable to any data source

Known limitations to IBM SQL Query:
------------------------------------------------

* SQL statement string size limit: 200KB
* Max concurrent SQL queries for a standard SQL Query instance is 5
* Max time for a query job is 1 hour. However, many jobs can be stopped at much earlier than that due to the current mechanism of AIM token timeout and this token is shared across all current SQL queries

Tips
-----

* Combine the SQL query if you can, as there is an overhead (and possibly $ cost) for a REST API request. However, also consider the current limit for a YARN executor is 7.5GB, so design the SQL query wisely. It is best if the data being accessed is organized with multiple objects, of ideal sizes (see below), since this enables more parallelism in the object store.
* Stored in Parquet format: complex data can only be stored using Json or Parquet. It's faster with Parquet. We however can't control the choice of compression algorithm.
* Avoid storing the data in that a single object's size is > 200MB. To check, consider using :meth:`.get_cos_summary` or :meth:`.list_results`. To resolve the issue, consider using

    + Partition table into multiple buckets/objects type-1: PARTITION INTO <x> BUCKETS/OBJECTS, with maximum allowed for 'x' is 50.
    + Partition table into multiple buckets/objects type-2: PARTITIONED EVERY <x> ROWS
    + Hive-style partitioning: PARTITION BY (col1, col2, ...)
* When partitioning according to a column that has NULL values, Spark will use “__HIVE_DEFAULT_PARTITION__” in the object name, e.g. <bucket>/Location=__HIVE_DEFAULT_PARTITION__/<data-partition>

.. code-block:: python

        sqlClient.list_results(job_id)

.. code-block:: console

        ObjectURL	Size	Bucket	Object
        0	cos://s3.us-south.cloud-object-storage.appdomain.cloud/sql-query-cos-access-ts/jobid=a3475263-469a-4e22-b382-1d0ae8f1d1fa	0	sql-query-cos-access-ts	jobid=a3475263-469a-4e22-b382-1d0ae8f1d1fa
        1	cos://s3.us-south.cloud-object-storage.appdomain.cloud/sql-query-cos-access-ts/jobid=a3475263-469a-4e22-b382-1d0ae8f1d1fa/_SUCCESS	0	sql-query-cos-access-ts	jobid=a3475263-469a-4e22-b382-1d0ae8f1d1fa/_SUCCESS
        2	cos://s3.us-south.cloud-object-storage.appdomain.cloud/sql-query-cos-access-ts/jobid=a3475263-469a-4e22-b382-1d0ae8f1d1fa/part-00000-e299e734-43e3-4032-b27d-b0d7e93d51c2-c000-attempt_20200318152159_0040_m_000000_0.snappy.parquet	7060033106	sql-query-cos-access-ts	jobid=a3475263-469a-4e22-b382-1d0ae8f1d1fa/part-00000-e299e734-43e3-4032-b27d-b0d7e93d51c2-c000-attempt_20200318152159_0040_m_000000_0.snappy.parquet


References
--------------

*  `sparksql-parser <https://github.ibm.com/SqlServiceWdp/sparksql-parser>`_: The module contains code that know how to parse a SQLCloud-specific statement and transform into valid SQL statement
* `grammar <https://github.ibm.com/SqlServiceWdp/sparksql-parser/blob/8895a3872790d21e4bb0f0e47a608bfb633e0b2a/antlr/SqlQuery.g4>`_: the grammar
* `tips for data layout <https://www.ibm.com/cloud/blog/big-data-layout>`_
* `data skipping <https://www.ibm.com/cloud/blog/data-skipping-for-ibm-cloud-sql-query>`_
